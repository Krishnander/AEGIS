{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56574cbf",
   "metadata": {},
   "source": [
    "# AEGIS â€” MedGemma Inference Server (Kaggle)\n",
    "\n",
    "This notebook loads **MedGemma 1.5 4B IT** on Kaggle's free T4 GPU and serves it\n",
    "as an OpenAI-compatible API via **ngrok**.\n",
    "\n",
    "## Setup\n",
    "1. **Accelerator**: GPU T4 x2 (Settings â†’ Accelerator)\n",
    "2. **Internet**: ON (Settings â†’ Internet â†’ Internet connected)\n",
    "3. **Secrets**: Add `HF_TOKEN` and `NGROK_TOKEN` in Add-ons â†’ Secrets\n",
    "4. Run all cells, copy the ngrok URL into your `.env.local`\n",
    "\n",
    "## Model\n",
    "- `google/medgemma-1.5-4b-it` â€” 4-bit quantized, fits in ~5 GB VRAM\n",
    "- Updated Jan 13, 2026: improved EHR, CT/MRI 3D imaging, longitudinal CXR, lab report extraction\n",
    "- Part of Google's HAI-DEF collection (required for competition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2affde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install dependencies\n",
    "%pip install -q -U transformers accelerate bitsandbytes\n",
    "%pip install -q fastapi uvicorn pyngrok nest_asyncio\n",
    "print(\"âœ… Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611823e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load MedGemma 1.5 4B IT (4-bit quantized)\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
    "\n",
    "# Kaggle Secrets (only available when running on Kaggle)\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    _secrets = UserSecretsClient()\n",
    "    HF_TOKEN = _secrets.get_secret(\"HF_TOKEN\")\n",
    "    NGROK_TOKEN = _secrets.get_secret(\"NGROK_TOKEN\")\n",
    "except ImportError:\n",
    "    # Fallback: read from environment variables (e.g. local testing)\n",
    "    HF_TOKEN = os.environ.get(\"HF_TOKEN\", \"\")\n",
    "    NGROK_TOKEN = os.environ.get(\"NGROK_TOKEN\", \"\")\n",
    "\n",
    "MODEL_ID = \"google/medgemma-1.5-4b-it\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # bfloat16 required â€” float16 causes NaN logits\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(f\"Loading {MODEL_ID} ...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID, token=HF_TOKEN)\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    token=HF_TOKEN,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "model.eval()\n",
    "print(f\"âœ… {MODEL_ID} loaded â€” {model.get_memory_footprint()/1e9:.1f} GB VRAM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96208a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Inference helper\n",
    "import json, re\n",
    "\n",
    "def generate(messages: list[dict], max_tokens: int = 1024, temperature: float = 0.0) -> str:\n",
    "    \"\"\"\n",
    "    Chat-completion style generation using MedGemma.\n",
    "\n",
    "    Uses processor.tokenizer.apply_chat_template which natively supports the\n",
    "    'system' role with plain string content â€” unlike processor.apply_chat_template\n",
    "    which is multimodal-only and does NOT support system role.\n",
    "\n",
    "    Reference: official RL notebook (google-health/medgemma) uses this exact pattern.\n",
    "    \"\"\"\n",
    "    # Normalise content: typed-dict lists -> plain strings\n",
    "    plain_messages = []\n",
    "    for msg in messages:\n",
    "        content = msg[\"content\"]\n",
    "        if isinstance(content, list):\n",
    "            content = \" \".join(\n",
    "                part.get(\"text\", \"\")\n",
    "                for part in content\n",
    "                if isinstance(part, dict) and part.get(\"type\") == \"text\"\n",
    "            )\n",
    "        plain_messages.append({\"role\": msg[\"role\"], \"content\": str(content)})\n",
    "\n",
    "    # Tokenise using the *tokenizer's* apply_chat_template (supports system role)\n",
    "    inputs = processor.tokenizer.apply_chat_template(\n",
    "        plain_messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=temperature > 0,\n",
    "            temperature=temperature if temperature > 0 else None,\n",
    "            top_p=0.95 if temperature > 0 else None,\n",
    "            pad_token_id=processor.tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    input_len = inputs[\"input_ids\"].shape[-1]\n",
    "    response = processor.tokenizer.decode(outputs[0][input_len:], skip_special_tokens=True)\n",
    "    return response.strip()\n",
    "\n",
    "\n",
    "# Quick smoke test â€” also verify system role is handled correctly\n",
    "test = generate([\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful medical assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What are 3 warning signs of a stroke?\"},\n",
    "], max_tokens=200)\n",
    "print(\"Smoke test:\", test[:400])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4d8eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: FastAPI server (OpenAI-compatible /v1/chat/completions)\n",
    "import time, uuid\n",
    "from fastapi import FastAPI, Request\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.responses import JSONResponse\n",
    "\n",
    "app = FastAPI(title=\"AEGIS MedGemma Server\")\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "\n",
    "def _content_str(content) -> str:\n",
    "    \"\"\"Coerce message content to a plain string for token counting.\"\"\"\n",
    "    if isinstance(content, list):\n",
    "        return \" \".join(\n",
    "            p.get(\"text\", \"\") for p in content\n",
    "            if isinstance(p, dict) and p.get(\"type\") == \"text\"\n",
    "        )\n",
    "    return str(content) if content else \"\"\n",
    "\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    return {\"status\": \"ok\", \"model\": MODEL_ID}\n",
    "\n",
    "\n",
    "@app.post(\"/v1/chat/completions\")\n",
    "async def chat_completions(request: Request):\n",
    "    body = await request.json()\n",
    "    messages = body.get(\"messages\", [])\n",
    "    max_tokens = body.get(\"max_tokens\", 1024)\n",
    "    temperature = body.get(\"temperature\", 0.0)\n",
    "\n",
    "    start = time.time()\n",
    "    content = generate(messages, max_tokens=max_tokens, temperature=temperature)\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    prompt_tokens = sum(\n",
    "        len(processor.tokenizer.encode(_content_str(m.get(\"content\", \"\"))))\n",
    "        for m in messages\n",
    "    )\n",
    "    completion_tokens = len(processor.tokenizer.encode(content))\n",
    "\n",
    "    return {\n",
    "        \"id\": f\"chatcmpl-{uuid.uuid4().hex[:8]}\",\n",
    "        \"object\": \"chat.completion\",\n",
    "        \"created\": int(time.time()),\n",
    "        \"model\": MODEL_ID,\n",
    "        \"choices\": [{\n",
    "            \"index\": 0,\n",
    "            \"message\": {\"role\": \"assistant\", \"content\": content},\n",
    "            \"finish_reason\": \"stop\",\n",
    "        }],\n",
    "        \"usage\": {\n",
    "            \"prompt_tokens\": prompt_tokens,\n",
    "            \"completion_tokens\": completion_tokens,\n",
    "            \"total_tokens\": prompt_tokens + completion_tokens,\n",
    "        },\n",
    "        \"latency_seconds\": round(elapsed, 2),\n",
    "    }\n",
    "\n",
    "\n",
    "# AEGIS-specific endpoint (simpler interface)\n",
    "@app.post(\"/analyze\")\n",
    "async def analyze(request: Request):\n",
    "    body = await request.json()\n",
    "    symptoms = body.get(\"symptoms\", \"\")\n",
    "    system_prompt = body.get(\n",
    "        \"systemPrompt\",\n",
    "        \"You are AEGIS, a clinical triage AI. Analyze the patient presentation \"\n",
    "        \"and return JSON with: symptoms, severity (low/medium/high), summary, \"\n",
    "        \"differential (list of {condition, probability, recommendation}), \"\n",
    "        \"recommendations, reasoning, confidence (0-1).\",\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Patient presentation: {symptoms}\"},\n",
    "    ]\n",
    "\n",
    "    content = generate(messages, max_tokens=1024, temperature=0.0)\n",
    "    return {\"response\": content, \"model\": MODEL_ID}\n",
    "\n",
    "\n",
    "print(\"âœ… FastAPI app defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff7c497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Launch server with ngrok tunnel\n",
    "import nest_asyncio\n",
    "import threading\n",
    "import uvicorn\n",
    "\n",
    "try:\n",
    "    from pyngrok import ngrok\n",
    "except ImportError:\n",
    "    raise ImportError(\"pyngrok not found â€” run Cell 1 first to install it.\")\n",
    "\n",
    "nest_asyncio.apply()\n",
    "ngrok.set_auth_token(NGROK_TOKEN)\n",
    "\n",
    "PORT = 8000\n",
    "\n",
    "# Start uvicorn in background thread\n",
    "thread = threading.Thread(\n",
    "    target=uvicorn.run,\n",
    "    args=(app,),\n",
    "    kwargs={\"host\": \"0.0.0.0\", \"port\": PORT, \"log_level\": \"info\"},\n",
    "    daemon=True,\n",
    ")\n",
    "thread.start()\n",
    "\n",
    "# Open ngrok tunnel\n",
    "public_url = ngrok.connect(PORT, \"http\").public_url\n",
    "print(\"=\" * 60)\n",
    "print(\"ðŸš€ AEGIS MedGemma Server is LIVE!\")\n",
    "print(f\"   Public URL: {public_url}\")\n",
    "print(f\"   Health:     {public_url}/health\")\n",
    "print(f\"   Analyze:    {public_url}/analyze\")\n",
    "print(f\"   OpenAI API: {public_url}/v1/chat/completions\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"ðŸ“‹ Add this to your .env.local:\")\n",
    "print(f\"   KAGGLE_MEDGEMMA_URL={public_url}\")\n",
    "print()\n",
    "print(\"Keep this notebook running while using AEGIS.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f855ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: (Optional) Test the endpoint\n",
    "import requests\n",
    "\n",
    "# Test health\n",
    "r = requests.get(f\"{public_url}/health\")\n",
    "print(\"Health:\", r.json())\n",
    "\n",
    "# Test analyze\n",
    "r = requests.post(f\"{public_url}/analyze\", json={\n",
    "    \"symptoms\": \"55 year old male with crushing chest pain radiating to left arm, diaphoresis, nausea\"\n",
    "})\n",
    "print(\"\\nAnalysis result:\")\n",
    "print(r.json()[\"response\"][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1c6e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Keep alive â€” run this cell to prevent notebook timeout\n",
    "import time\n",
    "print(\"Keeping server alive... (Interrupt kernel to stop)\")\n",
    "while True:\n",
    "    time.sleep(60)\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] Server running at {public_url}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
